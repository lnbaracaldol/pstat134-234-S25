d['agree_ratio'] = d['agree'] / d['disagree']
d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])
data.append(d)
sleep(10)
d
data
data= []
for row in rows:
d = dict()
d['name'] = row.select_one('.source-title').text.strip()
d['allsides_page'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']
d['bias'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1]
d['agree'] = int(row.select_one('.agree').text)
d['disagree'] = int(row.select_one('.disagree').text)
d['agree_ratio'] = d['agree'] / d['disagree']
#d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])
d['agreeance_text'] = row.select_one('.commtext').text.strip().replace("Community ", "") if row.select_one('.commtext') else None
data.append(d)
data= []
for row in rows:
d = dict()
d['name'] = row.select_one('.source-title').text.strip()
d['allsides_page'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']
d['bias'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1]
d['agree'] = int(row.select_one('.agree').text)
d['disagree'] = int(row.select_one('.disagree').text)
d['agree_ratio'] = d['agree'] / d['disagree']
#d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])
d['agreeance_text']= row.select_one('.commtext').text.strip().replace("Community ", "")
data.append(d)
data= []
for row in rows:
d = dict()
d['name'] = row.select_one('.source-title').text.strip()
d['allsides_page'] = 'https://www.allsides.com' + row.select_one('.source-title a')['href']
d['bias'] = row.select_one('.views-field-field-bias-image a')['href'].split('/')[-1]
d['agree'] = int(row.select_one('.agree').text)
d['disagree'] = int(row.select_one('.disagree').text)
d['agree_ratio'] = d['agree'] / d['disagree']
#d['agreeance_text'] = get_agreeance_text(d['agree_ratio'])
d['agreeance_text']= row.select_one('.commtext').text.strip().replace("Community ", "")
data.append(d)
data
print(row.select_one('.community-feedback-rating-page'))
print(data[0])
#print(row.select_one('.community-feedback-rating-page'))
print(row.select_one('.commtext').text.strip().replace("Community ", ""))
files = !wget -q -O - https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi | grep 'zipcode.zip' | sed 's/<a data/\n<a data/g' | grep -Po '(?<=href=")[^"]*(?=")'
files  ## file names from bash is in python variable!  WHat if we removed -O -
files = !wget -q -O - https://www.irs.gov/statistics/soi-tax-stats-individual-income-tax-statistics-zip-code-data-soi
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
options = Options()
options.headless = True
# Replace with your chromedriver path if needed
driver = webdriver.Chrome(options=options)
driver.get("https://www.forbes.com/top-colleges/")
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# Wait until the college names are present
WebDriverWait(driver, 10).until(
EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".rankings-table__name"))
)
# Parse rendered page
soup = BeautifulSoup(driver.page_source, "html.parser")
# Extract first 5 college names
colleges = [el.text.strip() for el in soup.select(".rankings-table__name")]
print(colleges[:5])
driver.quit()
print(colleges[:5])
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
options = Options()
options.headless = True
# Replace with your chromedriver path if needed
driver = webdriver.Chrome(options=options)
driver.get("https://www.forbes.com/top-colleges/")
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
WebDriverWait(driver, 10).until(
EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".rankings-table__name"))
)
# Parse rendered page
soup = BeautifulSoup(driver.page_source, "html.parser")
# Extract first 5 college names
colleges = [el.text.strip() for el in soup.select(".rankings-table__name")]
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
options = Options()
options.headless = True
# Replace with your chromedriver path if needed
driver = webdriver.Chrome(options=options)
driver.get("https://www.forbes.com/top-colleges/")
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
# Wait until the college names are present
WebDriverWait(driver, 10).until(
EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".rankings-table__name"))
)
# Parse rendered page
soup = BeautifulSoup(driver.page_source, "html.parser")
# Extract first 5 college names
colleges = [el.text.strip() for el in soup.select(".rankings-table__name")]
print(colleges[:5])
driver.quit()
reticulate::repl_python()
library(reticulate)
use_python("/Users/laurabaracaldo/miniconda3/envs/pstat134-234/bin/python", required = TRUE)
py_config()
reticulate::repl_python()
library(keras)
library(keras)
# OR truth table
dat <- list(
x = matrix(c(0,0,0,1,1,0,1,1), ncol=2, byrow=TRUE),
y = c(0,1,1,1)
)
model <- keras_model_sequential() %>%
layer_dense(units=1, input_shape=2, activation='sigmoid')
reticulate::repl_python()
library(keras)
# OR truth table
dat <- list(
x = matrix(c(0,0,0,1,1,0,1,1), ncol=2, byrow=TRUE),
y = c(0,1,1,1)
)
model <- keras_model_sequential() %>%
layer_dense(units=1, input_shape=2, activation='sigmoid')
library(keras)
# OR truth table
dat <- list(
x = matrix(c(0,0,0,1,1,0,1,1), ncol=2, byrow=TRUE),
y = c(0,1,1,1)
)
model <- keras_model_sequential() %>%
layer_dense(units=1, input_shape=2, activation='sigmoid')
model %>% compile(
optimizer = optimizer_sgd(),
loss = 'binary_crossentropy',
metrics = 'accuracy'
)
# 2. Prepare the data
set.seed(123)
dat <- list(
x = matrix(c(
0, 0,
0, 1,
1, 0,
1, 1
), ncol = 2, byrow = TRUE),
y = c(0, 1, 1, 1)
)
# 3. Define the model
model <- keras_model_sequential() %>%
layer_dense(
units        = 1,
activation   = "sigmoid",
input_shape  = c(2)        # specify as a vector
)
reticulate::repl_python()
library(keras3)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
str(train_images)
str(train_labels)
network <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
str(train_images)
str(train_labels)
network <- keras_model_sequential() %>%
layer_dense(units = 512, activation = "relu", input_shape = c(28 * 28)) %>%
layer_dense(units = 10, activation = "softmax")
network %>% compile(
optimizer = "rmsprop",
loss = "categorical_crossentropy",
metrics = c("accuracy")
)
train_images <- array_reshape(train_images, c(60000, 28 * 28))
train_images <- train_images / 255
test_images <- array_reshape(test_images, c(10000, 28 * 28))
test_images <- test_images / 255
train_labels <- to_categorical(train_labels)
test_labels <- to_categorical(test_labels)
network %>% fit(train_images, train_labels, epochs = 5, batch_size = 128)
metrics <- network %>% evaluate(test_images, test_labels)
metrics
network %>% predict_classes(test_images[1:10,])
network %>% predict_classes(test_images[1:10,])
network %>% predict(test_images[1:10,])
test_labels[1:10]
network %>% predict(test_images[1:10,]) %>% k_argmax()
network %>% predict(test_images[1:10,]) %>% k_argmax()
network %>% predict(test_images[1:10,])
preds<- network %>% predict(test_images[1:10,]) %>% k_argmax()
preds<- network %>% predict(test_images[1:10,])
classes_tensor <- k_argmax(preds)
classes_tensor <- keras3::op_argmax(preds, axis = -1L)
classes_tensor
classes <- as.array(classes_tensor)
classes
digit <- train_images[5,,]
plot(as.raster(digit, max = 255))
dim(train_images)
library(keras3)
mnist <- dataset_mnist()
train_images <- mnist$train$x
train_labels <- mnist$train$y
test_images <- mnist$test$x
test_labels <- mnist$test$y
digit <- train_images[5,,]
plot(as.raster(digit, max = 255))
x <- matrix(
c(0,0,
0,1,
1,0,
1,1),
ncol = 2,
byrow = TRUE
)
y <- c(0, 1, 1, 1)
# 2. One‐hot encode for 2 classes
y_cat <- to_categorical(y, num_classes = 2)
# 3. Define the network
network <- keras_model_sequential() %>%
layer_dense(
units       = 8,        # a small hidden layer
activation  = "relu",
input_shape = c(2)
) %>%
layer_dense(
units      = 2,         # output for 2 classes
activation = "softmax"
)
network %>% compile(
optimizer = "sgd",
loss      = "categorical_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
# 6. Evaluate
metrics <- network %>% evaluate(x, y_cat, verbose = 0)
cat(
"Final loss =", metrics["loss"],
"– Final accuracy =", metrics["accuracy"], "\n"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
history
x
# 6. Evaluate
metrics <- network %>% evaluate(x, y_cat, verbose = 0)
metrics
metrics["loss"]
metrics["accuracy"]
cat(
"Final loss =", metrics["loss"],
"– Final accuracy =", metrics["accuracy"], "\n"
)
list(
"Final loss =", metrics["loss"],
"– Final accuracy =", metrics["accuracy"], "\n"
)
preds           <- network %>% predict(x)
classes_tensor <- keras3::op_argmax(preds, axis = -1L)
classes        <- as.array(classes_tensor)
print(classes)
network <- keras_model_sequential() %>%
layer_dense(
units       = 2,         # directly output 2 class scores
activation  = "softmax",
input_shape = c(2)
)
network <- keras_model_sequential() %>%
layer_dense(
units       = 2,         # directly output 2 class scores
activation  = "softmax",
input_shape = c(2)
)
network %>% compile(
optimizer = "sgd",
loss      = "categorical_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
history
network <- keras_model_sequential() %>%
layer_dense(
units       = 2,         # directly output 2 class scores
activation  = "softmax",
input_shape = c(2)
)
network %>% compile(
optimizer = optimizer_adam(learning_rate = 0.001),
loss      = "categorical_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
history
network %>% compile(
optimizer = optimizer_adam(learning_rate = 0.1),
loss      = "categorical_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
history
# 3. Define the network
network <- keras_model_sequential() %>%
layer_dense(
units       = 8,        # a small hidden layer
activation  = "relu",
input_shape = c(2)
) %>%
layer_dense(
units      = 2,         # output for 2 classes
activation = "softmax"
)
network %>% compile(
optimizer = optimizer_adam(learning_rate = 0.1),
loss      = "categorical_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
history
# 6. Evaluate
metrics <- network %>% evaluate(x, y_cat, verbose = 0)
metrics
?compile
# OR truth table
dat <- list(
x = matrix(c(0,0,0,1,1,0,1,1), ncol=2, byrow=TRUE),
y = c(0,1,1,1)
)
model <- keras_model_sequential() %>%
layer_dense(units=1, input_shape=2, activation='sigmoid')
model %>% compile(
optimizer = optimizer_sgd(),
loss = 'binary_crossentropy',
metrics = 'accuracy'
)
model %>% fit(dat$x, dat$y, epochs=100, verbose=0)
model
print(model %>% predict(dat$x))
?layer_dense
network <- keras_model_sequential() %>%
layer_dense(
units       = 1,         # directly output 2 class scores
activation  = "sigmoid",
input_shape = c(2)
)
network %>% compile(
optimizer = optimizer_adam(learning_rate = 0.1),
loss      = "categorical_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
network <- keras_model_sequential() %>%
layer_dense(
units       = 2,         # directly output 2 class scores
activation  = "sigmoid",
input_shape = c(2)
)
network %>% compile(
optimizer = optimizer_adam(learning_rate = 0.1),
loss      = "binary_crossentropy",
metrics   = "accuracy"
)
# 5. Fit on the entire OR table
history <- network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
# 5. Fit on the entire OR table
network %>% fit(
x, y_cat,
epochs     = 200,
batch_size = 4,
verbose    = 0
)
history
# 6. Evaluate
metrics <- network %>% evaluate(x, y_cat, verbose = 0)
metrics
reticulate::repl_python()
## Activation (ReLU)
relu <- function(x) pmax(0, x)
relu_derivative <- function(x) as.numeric(x > 0)
# Data
x_data <- c(0.5, 2.0, -1.5)
y_data <- c(1.0, 4.0, -2.0)
# Initialize weights
set.seed(42)
w1 <- rnorm(1)
w2 <- rnorm(1)
b1 <- rnorm(1)
b2 <- rnorm(1)
w3 <- rnorm(1)
w4 <- rnorm(1)
b3 <- rnorm(1)
# Learning rate
lr <- 0.01
### Training
for (epoch in 1:500) {
loss_epoch <- 0
for (i in 1:length(x_data)) {
x <- x_data[i]
y <- y_data[i]
### Forward pass
z1 <- w1 * x + b1
z2 <- w2 * x + b2
h1 <- relu(z1)
h2 <- relu(z2)
y_hat <- w3 * h1 + w4 * h2 + b3
### Loss
loss <- 0.5 * (y_hat - y)^2
loss_epoch <- loss_epoch + loss
# Backward
dL_dyhat <- y_hat - y
dL_dw3 <- dL_dyhat * h1
dL_dw4 <- dL_dyhat * h2
dL_db3 <- dL_dyhat
dL_dh1 <- dL_dyhat * w3
dL_dh2 <- dL_dyhat * w4
dL_dz1 <- dL_dh1 * relu_derivative(z1)
dL_dz2 <- dL_dh2 * relu_derivative(z2)
dL_dw1 <- dL_dz1 * x
dL_db1 <- dL_dz1
dL_dw2 <- dL_dz2 * x
dL_db2 <- dL_dz2
### Update
w1 <- w1 - lr * dL_dw1
b1 <- b1 - lr * dL_db1
w2 <- w2 - lr * dL_dw2
b2 <- b2 - lr * dL_db2
w3 <- w3 - lr * dL_dw3
w4 <- w4 - lr * dL_dw4
b3 <- b3 - lr * dL_db3
}
if (epoch %% 50 == 0) {
print(paste("Epoch", epoch, "Loss:", loss_epoch))
}
}
print(paste("Final weights: w1 =", w1, ", w2 =", w2, ", w3 =", w3, ", w4 =", w4))
reticulate::repl_python()
