---
title: "Lab 4: Gradient Descent on Simple Functions"
author: "PSTAT 134/234"
format: html
editor: visual
---

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(reticulate)
use_condaenv("pstat134-234", required = TRUE) #specify here your env
```

## Introduction

In this lab, we explore **Gradient Descent (GD)** on simple functions and **build a Neural Network manually**. 
We implement everything manually in **R** and **Python**.



##  Objective

- Understand gradient descent intuitively
- Apply GD to minimize simple functions
- Build a basic neural network manually
- Visualize the optimization trajectory



## Part 1: Gradient Descent on a Quadratic Function


### Function

$$
f(x) = (x-3)^2
$$

### Derivative

$$
f'(x) = 2(x-3)
$$

We want to find the minimum of $f(x)$.



## Python Code

```{python}
import numpy as np
import matplotlib.pyplot as plt

# Function and derivative
def f(x):
    return (x - 3)**2

def df(x):
    return 2 * (x - 3)

# Gradient Descent
x = np.random.randn()
learning_rate = 0.1
n_iters = 50
x_hist = [x]
loss_hist = [f(x)]

for i in range(n_iters):
    grad = df(x)
    x = x - learning_rate * grad
    x_hist.append(x)
    loss_hist.append(f(x))

plt.plot(x_hist, f(np.array(x_hist)), 'o-')
plt.xlabel('x')
plt.ylabel('f(x)')
plt.title('Gradient Descent on f(x) = (x-3)^2')
plt.show()

print(f"Minimum at x = {x}")

```



## R Code

```{r}
# Function and derivative
f <- function(x) (x - 3)^2
df <- function(x) 2 * (x - 3)

# Gradient Descent
set.seed(42)
x <- rnorm(1)
learning_rate <- 0.1
n_iters <- 50
x_hist <- numeric(n_iters)

for (i in 1:n_iters) {
  x <- x - learning_rate * df(x)
  x_hist[i] <- x
}

plot(x_hist, f(x_hist), type="o", xlab="x", ylab="f(x)", main="Gradient Descent on f(x) = (x-3)^2")
print(paste("Minimum at x =", x))
```


## Part 2: Building a Neural Network Manually

We now manually implement a small neural network:

- **Input layer**: 1 input
- **Hidden layer**: 2 neurons (ReLU activation)
- **Output layer**: 1 neuron (linear activation)



## Mathematical Setup

- Input: $x$
- Hidden activations:

$$
h_1 = \sigma(w_1 x + b_1), \quad h_2 = \sigma(w_2 x + b_2)
$$

- Output:

$$
\hat{y} = w_3 h_1 + w_4 h_2 + b_3
$$

where $\sigma$ is the ReLU activation.

- Loss (MSE):

$$
L = \frac{1}{2} (\hat{y} - y)^2
$$



##  Python Code for Manual NN

```{python}
import numpy as np

####Activation (ReLU)
def relu(x):
    return np.maximum(0, x)

def relu_derivative(x):
    return (x > 0).astype(float)

###Data
x_data = np.array([0.5, 2.0, -1.5])
y_data = np.array([1.0, 4.0, -2.0])

###Initialize weights
np.random.seed(42)
w1, w2 = np.random.randn(2)
b1, b2 = np.random.randn(2)
w3, w4, b3 = np.random.randn(3)

##Learning rate
lr = 0.01

### Training
for epoch in range(500):
    loss_epoch = 0
    for x, y in zip(x_data, y_data):
        ## Forward pass
        z1 = w1 * x + b1
        z2 = w2 * x + b2
        h1 = relu(z1)
        h2 = relu(z2)
        y_hat = w3 * h1 + w4 * h2 + b3

        ##Loss
        loss = (y_hat - y)**2
        loss_epoch += loss

        ###Backward
        dL_dyhat = 2*(y_hat - y)
        dL_dw3 = dL_dyhat * h1
        dL_dw4 = dL_dyhat * h2
        dL_db3 = dL_dyhat

        dL_dh1 = dL_dyhat * w3
        dL_dh2 = dL_dyhat * w4

        dL_dz1 = dL_dh1 * relu_derivative(z1)
        dL_dz2 = dL_dh2 * relu_derivative(z2)

        dL_dw1 = dL_dz1 * x
        dL_db1 = dL_dz1
        dL_dw2 = dL_dz2 * x
        dL_db2 = dL_dz2

        ###Update parametesr
        w1 -= lr * dL_dw1
        b1 -= lr * dL_db1
        w2 -= lr * dL_dw2
        b2 -= lr * dL_db2
        w3 -= lr * dL_dw3
        w4 -= lr * dL_dw4
        b3 -= lr * dL_db3

    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss: {loss_epoch}")

print(f"Final Weights:\n w1={w1:.4f}, w2={w2:.4f}, w3={w3:.4f}, w4={w4:.4f}")

```

Now we perform prediction:


```{python}
####Predict after training
z1 = w1 * x_data + b1
z2 = w2 * x_data + b2
h1 = relu(z1)
h2 = relu(z2)
predictions = w3 * h1 + w4 * h2 + b3

print("Predicted responses (y_hat):", predictions)
```

## R Code for Manual NN

```{r}
## Activation (ReLU)
relu <- function(x) pmax(0, x)
relu_derivative <- function(x) as.numeric(x > 0)

###Data
x_data <- c(0.5, 2.0, -1.5)
y_data <- c(1.0, 4.0, -2.0)

###Initialize weights
set.seed(42)
w1 <- rnorm(1)
w2 <- rnorm(1)
b1 <- rnorm(1)
b2 <- rnorm(1)
w3 <- rnorm(1)
w4 <- rnorm(1)
b3 <- rnorm(1)

###Learning rate
lr <- 0.01

###Training
for (epoch in 1:500) {
  loss_epoch <- 0
  for (i in 1:length(x_data)) {
    x <- x_data[i]
    y <- y_data[i]

    ###Forward pass
    z1 <- w1 * x + b1
    z2 <- w2 * x + b2
    h1 <- relu(z1)
    h2 <- relu(z2)
    y_hat <- w3 * h1 + w4 * h2 + b3

    ###Loss
    loss <- (y_hat - y)^2
    loss_epoch <- loss_epoch + loss

    ###Backward
    dL_dyhat <- 2*(y_hat - y)
    dL_dw3 <- dL_dyhat * h1
    dL_dw4 <- dL_dyhat * h2
    dL_db3 <- dL_dyhat

    dL_dh1 <- dL_dyhat * w3
    dL_dh2 <- dL_dyhat * w4

    dL_dz1 <- dL_dh1 * relu_derivative(z1)
    dL_dz2 <- dL_dh2 * relu_derivative(z2)

    dL_dw1 <- dL_dz1 * x
    dL_db1 <- dL_dz1
    dL_dw2 <- dL_dz2 * x
    dL_db2 <- dL_dz2

    ### Update
    w1 <- w1 - lr * dL_dw1
    b1 <- b1 - lr * dL_db1
    w2 <- w2 - lr * dL_dw2
    b2 <- b2 - lr * dL_db2
    w3 <- w3 - lr * dL_dw3
    w4 <- w4 - lr * dL_dw4
    b3 <- b3 - lr * dL_db3
  }

  if (epoch %% 50 == 0) {
    print(paste("Epoch", epoch, "Loss:", loss_epoch))
  }
}

print(paste("Final weights: w1 =", w1, ", w2 =", w2, ", w3 =", w3, ", w4 =", w4))
```

Now we carry put the predistion of the response:

```{r}
### Predict after training
z1 <- w1 * x_data + b1
z2 <- w2 * x_data + b2
h1 <- relu(z1)
h2 <- relu(z2)
predictions <- w3 * h1 + w4 * h2 + b3

print("Predicted responses (y_hat):")
print(predictions)
```

## Part 3: Probit Classification with Single-Layer Neural Network

We now implement a simple one-layer neural network:

- **Input layer**: 1 input
- **Output layer**: 1 neuron
- **Activation**: **Probit (Normal CDF)**



## Mathematical Setup

- Input: $x$
- Output raw score:

$$
z = wx + b
$$

- Apply Probit:

$$
\hat{p} = \Phi(z)
$$

where $\Phi$ is the standard normal CDF.

- Loss (binary cross-entropy):

$$
L = -\left[y \log(\hat{p}) + (1-y) \log(1-\hat{p})\right]
$$



## Python Code for Probit NN

```{python}
import numpy as np
import scipy.stats as stats

######Binary data, continuous predictor
x_data = np.array([0.5, 2.0, -1.5, 1.2])
y_data = np.array([1, 1, 0, 1])

###We initialize weights
np.random.seed(42)
w = np.random.randn()
b = np.random.randn()

###Learning rate
lr = 0.1
loss_history = []

###Training
for epoch in range(300):
    loss_epoch = 0
    for x, y in zip(x_data, y_data):
        #####Forward pass
        z = w * x + b
        p = stats.norm.cdf(z)

        ###Loss
        loss = -(y * np.log(p) + (1-y) * np.log(1-p))
        loss_epoch += loss

        ####Backward
        phi = stats.norm.pdf(z) ##dPhat_dz
        dL_dz = (p - y) / (p * (1-p)) * phi

        ###Gradients
        dL_dw = dL_dz * x
        dL_db = dL_dz

        ####new weights
        w -= lr * dL_dw
        b -= lr * dL_db

    loss_history.append(loss_epoch)
    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss: {loss_epoch}")

print(f"Final weights: w={w:.4f}, b={b:.4f}")



```

After training, we predict the class labels based on the output probability. We use a threshold of 0.5: 



$$
\hat{y} = \begin{cases} 1 & \text{if } \hat{p} > 0.5 \\ 0 & \text{otherwise} \end{cases}
$$

```{python}
### Predicting class labels
preds = stats.norm.cdf(w * x_data + b)
predicted_classes = (preds >= 0.5).astype(int)
print("Predicted probabilities:", preds)
print("Predicted classes:", predicted_classes)
```

## Questions

1. Plot the loss path (loss vs epoch). Is it decreasing steadily?
2. Play around with the **learning rate** (try smaller/larger values).
3. What happens if you initialize the weights with very large/small values?
4. Compare the **predicted classes** with the true labels: how many were correct?
5. Try setting a threshold different from 0.5 (e.g., 0.6) for classification. Does it change predictions?